{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9Xj+zt5zhLZiH0fWnEsp6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"N42AD-_XaCJ2"},"outputs":[],"source":["from unstructured.partition.pdf import partition_pdf\n","import vertexai\n","\n","from vertexai.generative_models import GenerativeModel, ChatSession\n","project_id = \"---project_id----\"\n","\n","vertexai.init(project=project_id, location=\"us-central1\")\n","from google.cloud import aiplatform\n","\n","from langchain_google_vertexai import VertexAI\n","from langchain.prompts import PromptTemplate\n","from langchain.schema.output_parser import StrOutputParser\n","from langchain_core.messages import AIMessage\n","from langchain_core.runnables import RunnableLambda\n","\n","import base64\n","from langchain_google_vertexai import ChatVertexAI\n","import os\n","\n","from langchain_core.messages import HumanMessage\n","import uuid\n","\n","from langchain_google_vertexai import VertexAIEmbeddings\n","from langchain.retrievers.multi_vector import MultiVectorRetriever\n","from langchain.schema.document import Document\n","from langchain.storage import InMemoryStore\n","from langchain.vectorstores import Chroma\n","\n","import io\n","import re\n","\n","from IPython.display import HTML, display\n","from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n","from PIL import Image\n","\n","image_path = \"/content/drive/MyDrive/SOC_2024/Images/\"\n","pdf_elements = partition_pdf(\n","    \"/content/drive/MyDrive/SOC_2024/Retrieval_Content/Learn_to_drive_Manual_cars_for_dummies.pdf\",\n","    chunking_strategy=\"by_title\",\n","    extract_images_in_pdf=True,\n","    max_characters=3000,\n","    new_after_n_chars=2800,\n","    combine_text_under_n_chars=2000,\n","    image_output_dir_path=image_path\n","    )\n","\n","# Categorize elements by type\n","def categorize_elements(raw_pdf_elements):\n","    \"\"\"\n","    Categorize extracted elements from a PDF into tables and texts.\n","    raw_pdf_elements: List of unstructured.documents.elements\n","    \"\"\"\n","    tables = []\n","    texts = []\n","    for element in raw_pdf_elements:\n","        if \"unstructured.documents.elements.Table\" in str(type(element)):\n","            tables.append(str(element))\n","        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n","            texts.append(str(element))\n","    return texts, tables\n","\n","texts, tables = categorize_elements(pdf_elements)\n","\n","# Generate summaries of text elements\n","def generate_text_summaries(texts, tables, summarize_texts=False):\n","    \"\"\"\n","    Summarize text elements\n","    texts: List of str\n","    tables: List of str\n","    summarize_texts: Bool to summarize texts\n","    \"\"\"\n","\n","    # Prompt\n","    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n","    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n","    Give a concise summary of the table or text that is well-optimized for retrieval. Table \\\n","    or text: {element} \"\"\"\n","    prompt = PromptTemplate.from_template(prompt_text)\n","    empty_response = RunnableLambda(\n","        lambda x: AIMessage(content=\"Error processing document\")\n","    )\n","\n","    # Text summary chain\n","    model = VertexAI(\n","        temperature=0, model_name=\"gemini-pro\", max_output_tokens=1024\n","    ).with_fallbacks([empty_response])\n","    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n","\n","    # Initialize empty summaries\n","    text_summaries = []\n","    table_summaries = []\n","\n","    # Apply to text if texts are provided and summarization is requested\n","    if texts and summarize_texts:\n","        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n","    elif texts:\n","        text_summaries = texts\n","\n","    # Apply to tables if tables are provided\n","    if tables:\n","        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n","\n","    # Filter out any blank or empty summaries\n","    text_summaries = [summary for summary in text_summaries if summary.strip()]\n","    table_summaries = [summary for summary in table_summaries if summary.strip()]\n","\n","    return text_summaries, table_summaries\n","\n","# Get text, table summaries\n","text_summaries2, table_summaries = generate_text_summaries(\n","    texts, tables, summarize_texts=True\n",")\n","\n","def encode_image(image_path):\n","    \"\"\"Getting the base64 string\"\"\"\n","    with open(image_path, \"rb\") as image_file:\n","        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n","\n","\n","def image_summarize(img_base64, prompt):\n","    \"\"\"Make image summary\"\"\"\n","    model = ChatVertexAI(model_name=\"gemini-pro-vision\", max_output_tokens=1024)\n","\n","    msg = model(\n","        [\n","            HumanMessage(\n","                content=[\n","                    {\"type\": \"text\", \"text\": prompt},\n","                    {\n","                        \"type\": \"image_url\",\n","                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n","                    },\n","                ]\n","            )\n","        ]\n","    )\n","    return msg.content\n","\n","def generate_img_summaries(path):\n","    \"\"\"\n","    Generate summaries and base64 encoded strings for images\n","    path: Path to list of .jpg files extracted by Unstructured\n","    \"\"\"\n","\n","    # Store base64 encoded images\n","    img_base64_list = []\n","\n","    # Store image summaries\n","    image_summaries = []\n","\n","    # Prompt\n","    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n","    These summaries will be embedded and used to retrieve the raw image. \\\n","    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n","\n","    # Apply to images\n","    for img_file in sorted(os.listdir(path)):\n","        if img_file.endswith(\".jpg\"):\n","            img_path = os.path.join(path, img_file)\n","            base64_image = encode_image(img_path)\n","            img_base64_list.append(base64_image)\n","            image_summaries.append(image_summarize(base64_image, prompt))\n","\n","    return img_base64_list, image_summaries\n","\n","fpath = \"/content/figures\"\n","# Image summaries\n","img_base64_list, image_summaries = generate_img_summaries(fpath)\n","\n","def create_multi_vector_retriever(\n","    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images\n","):\n","    \"\"\"\n","    Create retriever that indexes summaries, but returns raw images or texts\n","    \"\"\"\n","\n","    # Initialize the storage layer\n","    store = InMemoryStore()\n","    id_key = \"doc_id\"\n","\n","    # Create the multi-vector retriever\n","    retriever = MultiVectorRetriever(\n","        vectorstore=vectorstore,\n","        docstore=store,\n","        id_key=id_key,\n","    )\n","    # Helper function to add documents to the vectorstore and docstore\n","    def add_documents(retriever, doc_summaries, doc_contents):\n","        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n","        summary_docs = [\n","            Document(page_content=s, metadata={id_key: doc_ids[i]})\n","            for i, s in enumerate(doc_summaries)\n","        ]\n","        retriever.vectorstore.add_documents(summary_docs)\n","        retriever.docstore.mset(list(zip(doc_ids, doc_contents)))\n","\n","    # Add texts, tables, and images\n","    # Check that text_summaries is not empty before adding\n","    if text_summaries:\n","        add_documents(retriever, text_summaries, texts)\n","    # Check that table_summaries is not empty before adding\n","    if table_summaries:\n","        add_documents(retriever, table_summaries, tables)\n","    # Check that image_summaries is not empty before adding\n","    if image_summaries:\n","        add_documents(retriever, image_summaries, images)\n","\n","    return retriever\n","\n","# The vectorstore to use to index the summaries\n","vectorstore = Chroma(\n","    collection_name=\"driving_info\",\n","    embedding_function=VertexAIEmbeddings(model_name=\"textembedding-gecko@latest\"),\n",")\n","\n","# Create retriever\n","retriever_multi_vector_img = create_multi_vector_retriever(\n","    vectorstore,\n","    text_summaries2,\n","    texts,\n","    table_summaries,\n","    tables,\n","    image_summaries,\n","    img_base64_list,\n",")\n","\n","def looks_like_base64(sb):\n","    \"\"\"Check if the string looks like base64\"\"\"\n","    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n","\n","\n","def is_image_data(b64data):\n","    \"\"\"\n","    Check if the base64 data is an image by looking at the start of the data\n","    \"\"\"\n","    image_signatures = {\n","        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n","        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n","        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n","        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n","    }\n","    try:\n","        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n","        for sig, format in image_signatures.items():\n","            if header.startswith(sig):\n","                return True\n","        return False\n","    except Exception:\n","        return False\n","\n","def resize_base64_image(base64_string, size=(128, 128)):\n","    \"\"\"\n","    Resize an image encoded as a Base64 string\n","    \"\"\"\n","    # Decode the Base64 string\n","    img_data = base64.b64decode(base64_string)\n","    img = Image.open(io.BytesIO(img_data))\n","\n","    # Resize the image\n","    resized_img = img.resize(size, Image.LANCZOS)\n","\n","    # Save the resized image to a bytes buffer\n","    buffered = io.BytesIO()\n","    resized_img.save(buffered, format=img.format)\n","\n","    # Encode the resized image to Base64\n","    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n","\n","def split_image_text_types(docs):\n","    \"\"\"\n","    Split base64-encoded images and texts\n","    \"\"\"\n","    b64_images = []\n","    texts = []\n","    for doc in docs:\n","        # Check if the document is of type Document and extract page_content if so\n","        if isinstance(doc, Document):\n","            doc = doc.page_content\n","        if looks_like_base64(doc) and is_image_data(doc):\n","            doc = resize_base64_image(doc, size=(1300, 600))\n","            b64_images.append(doc)\n","        else:\n","            texts.append(doc)\n","    if len(b64_images) > 0:\n","        return {\"images\": b64_images[:1], \"texts\": []}\n","    return {\"images\": b64_images, \"texts\": texts}\n","\n","def img_prompt_func(data_dict):\n","    \"\"\"\n","    Join the context into a single string\n","    \"\"\"\n","    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n","    messages = []\n","\n","    # Adding the text for analysis\n","    text_message = {\n","        \"type\": \"text\",\n","        \"text\": (\n","            \"You are an AI scientist tasking with providing factual answers.\\n\"\n","            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n","            \"Use this information to provide answers related to the user question. \\n\"\n","            f\"User-provided question: {data_dict['question']}\\n\\n\"\n","            \"Text and / or tables:\\n\"\n","            f\"{formatted_texts}\"\n","        ),\n","    }\n","    messages.append(text_message)\n","    # Adding image(s) to the messages if present\n","    if data_dict[\"context\"][\"images\"]:\n","        for image in data_dict[\"context\"][\"images\"]:\n","            image_message = {\n","                \"type\": \"image_url\",\n","                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n","            }\n","            messages.append(image_message)\n","    return [HumanMessage(content=messages)]\n","\n","def multi_modal_rag_chain(retriever):\n","    \"\"\"\n","    Multi-modal RAG chain\n","    \"\"\"\n","\n","    # Multi-modal LLM\n","    model = ChatVertexAI(\n","        temperature=0, model_name=\"gemini-pro-vision\", max_output_tokens=1024\n","    )\n","\n","    # RAG pipeline\n","    chain = (\n","        {\n","            \"context\": retriever | RunnableLambda(split_image_text_types),\n","            \"question\": RunnablePassthrough(),\n","        }\n","        | RunnableLambda(img_prompt_func)\n","        | model\n","        | StrOutputParser()\n","    )\n","\n","    return chain\n","\n","\n","# Create RAG chain\n","chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)\n","query = \"\"\"what are gears?\"\"\"\n","docs = retriever_multi_vector_img.get_relevant_documents(query, limit=1)\n","\n","print(docs[0])\n","print(chain_multimodal_rag.invoke(query))\n"]}]}